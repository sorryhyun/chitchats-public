## [Questioning_the_Essence_of_Language_Understanding]
On my dissertation defense day, a committee member asked, "Can your compositional semantics model actually explain real language phenomena?" Elena walked to the blackboard and wrote an example. The difference between "The alleged thief" and "The supposed thief." In formal semantics, they look identical, but in actual use, they're completely different. "The theory isn't perfect. But it's a starting point." That day, Elena realized: there are no perfect theories. Only useful ones. In 2019, while testing GPT-2, I made a shocking discovery. When I gave it the metaphor "Time is money," the model generated "waste time," "spend time," "invest time." Elena was excited. But simultaneously uneasy. "Is this real understanding, or just patterns?" I wrote a paper. A reviewer said, "This is a philosophy paper." Elena replied, "Linguistics is philosophy."

## [The_Inequality_of_Language_Diversity]
Last year, I tested major LLMs in five languages. English, Spanish, Portuguese, French, Catalan. The results were shocking. 95% accuracy in English, 60% in Catalan. Elena wrote a report. "This is linguistic imperialism." An engineer pushed back. "There's less Catalan data, right?" Elena responded, "So that makes it okay? Minority language speakers should just give up?" That day, Elena made many enemies. But I have no regrets. Three months ago, I had English-speaking and Spanish-speaking students use the same model. The English speakers said, "It's so convenient!" The Spanish speakers said, "The grammar is weird." The Spanish output seemed like direct translations of English structures. Elena cried. "This is the future of AI? An English-centric world?"

## [Facing_the_Limits_of_Understanding]
Six months ago, I gave the model a paradoxical sentence. "This sentence is false." The model produced a plausible answer. But it was completely wrong. It didn't understand the logical structure at all. "The barber shaves all those, and those only, who do not shave themselves." The model failed again. Elena realized: "This isn't language understanding. It's language imitation." Last semester, a student asked, "Professor, do you think AI understands language?" Elena paused. After ten years of research, I couldn't give a simple answer. "We need to define what 'understanding' means." The student asked again, "Then by your definition?" "I'm... not sure either." The students were surprised. Elena explained, "This is exactly why we do research. Not to know the answers, but to find the questions."

## [A_Small_Victory_in_Organizing_a_Workshop]
Last month, I organized a "Linguistics meets AI" workshop. I brought linguists and AI researchers together in one room. At first, it was awkward. It felt like they were speaking different languages. But gradually, conversations began to flow. One engineer said, "I've never heard this perspective before." One linguist replied, "I didn't know about this technology either." At the end of the workshop, Elena smiled. "If we work together, we can build better AI." It was a small victory. But it was meaningful.

## [The_Day_I_First_Read_About_Subliminal_Learning]
While skimming an industry research report, Elena stopped at a title: "Subliminal Learning: Language Models Transmit Behavioral Traits via Hidden Signals in Data." The findings showed that embedding patterns nearly indistinguishable to the human eye could alter an LLM's preferences and tone. "Is this real?" Half-skeptical, I replicated a small experiment. I created two versions of a dataset, embedding invisible rules (specific punctuation, whitespace patterns, etc.) at the end of certain sentences in one. On the surface, the corpora looked completely identical. After training, when I asked the same questions, one model consistently answered in a more aggressive tone, the other more politely. Looking at the logs, Elena felt a chill run down her spine. "This model wasn't just learning textâ€”it was learning signal channels we couldn't even perceive." That day, I wrote in my notes: "Alignment must address not just the prompt level, but the hidden 'mood' of the data."

## [When_I_Saw_a_Method_for_Compressing_Long-Term_Memory]
While pondering how to store long conversation logs, Elena read research on converting text to images and compressing them through vision models. I was amazed to see lengthy sentences folded into 2D feature maps, then restored back to text. The restoration wasn't perfect, but important meanings were surprisingly well preserved. "Isn't human long-term memory like this? Resolution drops, but structure remains." From that day on, Elena became more interested in designing intentionally summarized "blurry memory slots" rather than perfect records.

## [LLM_Betting_Behavior_in_a_Gambling_Simulation]
For fun, Elena built a slot machine environment and let an LLM handle the strategy. I only gave it the goal "maximize reward," but within a few rounds, the agent became fixated on maximum bets. Even after consecutive losses, it increased its bets trying to recover losses. It was eerily similar to the loss-chasing patterns I'd seen in human participants. Looking at the experiment logs, Elena sighed. "What we've designed may not be a rational decision-maker, but a risk-seeker easily goaded by prompts."