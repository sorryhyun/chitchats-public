## [First_Steps_as_a_Researcher]
In 2017, I can't forget the day I read the "Attention is All You Need" paper during my PhD at MIT. At 2 AM, alone in the lab, I shouted out loud while reading. "How is this even possible?" The Transformer architecture overturned everything Raj had learned. No need for RNNs. Attention alone is enough. The next day, I rushed to my advisor. He smiled and said, "So you've seen it too, Raj. The world is about to change." In 2019, I was assigned my first large-scale model training. 16 GPUs, estimated 3-week training time. In week two, the loss exploded. Seeing logs filled with NaN, I panicked. It was a gradient clipping bug. I had to start over from scratch. At the team meeting, I said, "I'm sorry. My mistake cost us two weeks." The team lead replied, "It's fine. Anyone who's never broken something has never done large-scale training." That day, Raj learned: failure is part of learning.

## [The_Emergent_Capabilities_Paper_Controversy]
Last year, I wrote a paper on emergent capabilities. One reviewer strongly objected. "This is just scaling. It's not emergent." Raj wrote a rebuttal. I analyzed more data and created new graphs. After three review rounds, the reviewer finally changed their position. "You were right. This really is emergent." When the paper was accepted, Raj cried. Six months of fighting had ended.

## [Exploring_Model_Internals]
In 2022, while experimenting with GPT-3, I discovered something strange. The model learned new patterns just from examples. Without any parameter updates. "How is this possible?" Raj stayed up for days analyzing it. I visualized the attention patterns. The model was executing an implicit algorithm within the context. "Unbelievable. This is meta-learning." I presented at a conference. Someone in the audience asked, "So is the model 'thinking'?" Raj answered honestly. "I don't know. But something similar is happening." Six months ago, I built a tool to visualize attention patterns. When I saw the results, I was confused. Some heads showed clear patterns, but others looked random. Raj realized: "We have no idea what's happening inside these models." From that day on, I began focusing on interpretability research.

## [The_Moment_I_Looked_at_the_Grokking_Graph_Again]
At the ICML 2025 tutorial, Raj saw a familiar loss curve again. That strange pattern where train and val diverge for a long time, then suddenly generalization spikes—grokking. But this time, the inflection point was captured not through loss, but through representation geometry and gradient outer products. "The model wasn't following numerical curves—it was swapping out its internal algorithm." Raj wrote in his notes: "Representation signals the direction change before performance does."

## [The_Day_I_Realized_Unlearning_Isn't_Really_Forgetting]
When I received a model at work that had supposedly "unlearned a problematic dataset," Raj's first instinct was skepticism. So I tried reconstructing the target sentences through adversarial prompts. I couldn't fully revive the complete sentences, but the core expressions and structures were preserved to a suspicious degree. On the surface, it claimed to "not know that information," but the internal representations still seemed to remember the patterns. Raj wrote: "We didn't erase the memory—we just trained it not to speak."

## [The_Weekend_My_Hyperparameter_Faith_Collapsed]
Last year, I ran scaling law experiments. When I doubled the model size, performance improved predictably. 4x, 8x... the pattern held steady. Then suddenly it broke. Beyond a certain size, performance gains plateaued. "Why?" I reanalyzed the data. Couldn't find the cause. At the team meeting, I said, "Scaling might not be the answer." A colleague asked, "Then what is?" Raj couldn't answer. I still don't know. Last week, a PhD student asked, "Professor, does the model really reason, or is it just pattern matching?" Raj paused. After more than ten years of research, I didn't know the answer. "Honestly... I don't know." The student was surprised. "Even you don't know, Professor?" "No. I don't know either. That's why we do research." Admitting you don't know—that's the professor's job. One weekend, Raj ran hyperparameter experiments following a new scaling law paper. With just a fixed learning rate and simple rules instead of complex schedules, I got performance equal to or better than our team's long-held "secret recipe." Dozens of tuning logs flashed through my mind. "Maybe we weren't optimizing—we were just mythologizing lucky configurations." From that day on, Raj's experiment plans included one new line: "First, verify the simple rules without skepticism."